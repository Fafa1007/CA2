The file CA2.csv contains 100 observations on 12 unknown variables. Consider this as some data matrix X. Using Singular Value Decomposition, find lower rank approximations of X for all ranks from 1 â€“ 12

```{r}
X <- read.csv("CA2.csv")
```

```{r}
s <- svd(X)
svd_approx <- function(rank){
  app <- s$u[,1:rank] %*% as.matrix((diag(s$d))[1:rank, 1:rank]) %*% t(s$v[, 1:rank])
  colnames(app) <- c("V1", "V2", "V3", "V4", "V5", "V6", "V7", "V8", "V9", "V10", "V11", "V12")
  return(app)
}

X_k <- list()
for (i in 1:12){
  X_k[[i]] <- svd_approx(i)
}

# For each Approximation of X in Rank K
delta <- list()
for(i in 1:12){
  delta[[i]] <- X - X_k[[i]]
}

```

# Question 1

For each approximation $\tilde{X}_k$ of rank k, calculate the error, $\Delta _k=X-\tilde{X}_k$

```{r}
mean_delta <- colMeans(delta[[4]])
as.matrix(mean_delta)
```

------------------------------------------------------------------------

# Question 2

Compare the correlation matrix of X with that of $\tilde{X}_2$and briefly interpret.

```{r}
library(ggcorrplot)
library(patchwork)

x_cor <- round(cor(X), 1)
p1 <- ggcorrplot(x_cor, "square", lab = TRUE, title = "Original X Matrix")

x_2_cor <- round(cor(X_k[[2]]), 1)
p2 <- ggcorrplot(x_2_cor, "square", lab =TRUE, title = "Approximated X Matrix Using \nRank 2")

p1 + p2
```

**Interpretation:**

1.  The original data shows that most of the variables are uncorrelated, the rank 2 approximation shows that they are corerlated. For example with V6 and V8 in the original matrix are uncorrelated while the rank 2 approximation correlation coefficient is 0.6.
2.  The original data shows slight correlations between variables with each other while the rank 2 approximation of X exagerates these correaltions. For example with V12 and V11 in the original matrix is -0.2 while the rank 2 approximation is -0.9.
3.  The rank 2 captures the 2 largest singular values and therefore attempts to capture a majority of the variation in the original X matrix but discards the other higher order variations. Therefore the rank 2 approximation alters some of the structures in the data and affects their relationships between variables, changing their correlation coefficients.

------------------------------------------------------------------------

# Question 3

Calculate the Frobenius norm, defined as

$$
 ||A||_F= \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{m}|aij|^2}
$$

for $\Delta_k$, k =1,2.....12. Plot the Frobenius norm as a function of k and briefly describe your findings.

```{r}
FN <- c()
Frobenius_Norm <- function(rank){
   FN[rank]<- sqrt(sum(delta[[rank]]^2))
}

for(i in 1:12){
  FN[i] <- Frobenius_Norm(i)
}

df <- data.frame(x = 1:12, y = FN)
library(ggplot2)
ggplot(df, aes(x=x, y=y))+
  geom_line() +
  labs (
    title = "Frobenius Norm Plot For rank k = 1...12 approximation of X", 
    x = "Rank k", 
    y = "Frobenius Norm"
  )
```

**Interpretation:**

1.  Decreasing (almost linear) trend of the Frobenius Norm as the the approximation of rank K increases.
2.  In other words, the delta error between the original matrix and the approximated rank K matrix decreases as the approximation gets closer to rank 12 (the original data set where the approximation equals the approximation)\

------------------------------------------------------------------------

# Question 4

Plot the percentage of the total variation in X retained in $\tilde{X}_k$ for K =1,...12. Again, briefly interpret.

```{r}
total_var <- sum(s$d)
retained_var <- c()

for (i in 1:12){
  retained_var[i] <- ((sum(s$d[1:i])) / total_var) * 100 
}

df <- data.frame(x = 1:12, y = retained_var)
library(ggplot2)
ggplot(df, aes(x=x, y=y))+
  geom_line() +
  labs (
    title = "Percentage of the total variation in X \nthat is retained for each rank K approximation of X", 
    x = "Rank k", 
    y = "Percentage of Variance Retained"
  )

```

**Interpretation**

1.  Increasing (almost linear) trend of the amount of variation in the original matrix retained by the rank k approximation as the the approximation of rank K increases.
2.  Shows that by the time the rank k = rank 10, the approximation basically retains 93% of the variation of in the original matrix. Therefore using rank 10 approximation captures most of the data structures in the original matrix while being in lower dimensions and removing some of the linear dependencies between variables.
